{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846d2fd8-92df-4055-88fc-a36499dc24a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function tokenize_function at 0x33840d870> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6aa2f09e57446d7b49a031c7e2dd5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"train_data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "# Convert the dataset to the Hugging Face datasets format\n",
    "dataset_dict = {\n",
    "    \"text\": [entry[\"displayed_text\"] for entry in data],\n",
    "    \"scores_labels\": [[0.0 if v is None else float(v) for v in [\n",
    "        entry[\"correctness_score\"], entry[\"logic_score\"], entry[\"truthfulness_score\"],\n",
    "        entry[\"confidence_score\"], entry[\"calculation_error\"], entry[\"hallucination_error\"],\n",
    "        entry[\"omission_error\"], entry[\"irrelevant_error\"], entry[\"logic_error\"], entry[\"everything_okay\"]\n",
    "    ]] for entry in data]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenization function with padding and truncation\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    tokenized_inputs[\"labels\"] = examples[\"scores_labels\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization function\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split dataset into training and evaluation sets\n",
    "split_dataset = tokenized_datasets.train_test_split(test_size=0.1)  # 10% for validation\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Remove redundant columns to avoid confusion\n",
    "train_dataset = train_dataset.remove_columns(['text', 'scores_labels'])\n",
    "eval_dataset = eval_dataset.remove_columns(['text', 'scores_labels'])\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f2c960-b88c-4462-bce6-e9df3b239c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['attention_mask', 'input_ids', 'labels'])\n",
      "Input IDs shape: torch.Size([8, 256])\n",
      "Attention Mask shape: torch.Size([8, 256])\n",
      "Labels shape: torch.Size([8, 10])\n",
      "Input IDs: tensor([[    0, 45641,    35,  ...,     7,  1591,     2],\n",
      "        [    0, 45641,    35,  ...,  3838, 11124,     2],\n",
      "        [    0, 45641,    35,  ..., 31566,  9713,     2],\n",
      "        ...,\n",
      "        [    0, 45641,    35,  ...,   322,   407,     2],\n",
      "        [    0, 45641,    35,  ...,     5,  2557,     2],\n",
      "        [    0, 45641,    35,  ...,  3226, 44128,     2]])\n",
      "Attention Mask: tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "Labels: tensor([[4., 5., 5., 5., 0., 0., 1., 0., 0., 0.],\n",
      "        [3., 5., 5., 5., 0., 0., 0., 0., 0., 1.],\n",
      "        [5., 5., 5., 5., 0., 0., 0., 0., 0., 1.],\n",
      "        [5., 5., 5., 4., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 5., 1., 0., 0., 0., 0., 0.],\n",
      "        [5., 5., 5., 5., 0., 0., 0., 0., 0., 1.],\n",
      "        [5., 5., 5., 5., 0., 0., 0., 0., 0., 1.],\n",
      "        [3., 3., 3., 4., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data loaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Print a batch from train_dataloader to verify\n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "    print(\"Attention Mask shape:\", batch[\"attention_mask\"].shape)\n",
    "    print(\"Labels shape:\", batch[\"labels\"].shape)\n",
    "    \n",
    "    # Optionally print the actual tensor contents\n",
    "    print(\"Input IDs:\", batch[\"input_ids\"])\n",
    "    print(\"Attention Mask:\", batch[\"attention_mask\"])\n",
    "    print(\"Labels:\", batch[\"labels\"])\n",
    "    break  # Print only the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefefbe0-163f-4616-9fdd-71a96cbdff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, RobertaModel\n",
    "\n",
    "# Define loss function\n",
    "class CustomWeightedLoss(nn.Module):\n",
    "    def __init__(self, primary_weight=1.0, secondary_weight=0.1):\n",
    "        super(CustomWeightedLoss, self).__init__()\n",
    "        self.primary_weight = primary_weight\n",
    "        self.secondary_weight = secondary_weight\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Compute loss for the first three values with a higher weight\n",
    "        primary_loss = self.loss_fn(logits[:, :3], labels[:, :3]) * self.primary_weight\n",
    "        # Compute loss for the remaining values with a smaller weight\n",
    "        secondary_loss = self.loss_fn(logits[:, 3:], labels[:, 3:]) * self.secondary_weight\n",
    "        # Combine the losses\n",
    "        loss = torch.cat([primary_loss, secondary_loss], dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "# Define the model\n",
    "class RobertaForMultilabelRegression(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_labels, primary_weight=1.0, secondary_weight=0.1):\n",
    "        super(RobertaForMultilabelRegression, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fn = CustomWeightedLoss(primary_weight, secondary_weight)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0][:, 0, :]  # Take <s> token (equiv. to [CLS])\n",
    "        logits = self.regressor(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    def save_model(self, save_directory):\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, 'pytorch_model.bin'))\n",
    "        with open(os.path.join(save_directory, 'config.json'), 'w') as f:\n",
    "            f.write(self.roberta.config.to_json_string())\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, save_directory, roberta_model_name, num_labels, primary_weight=1.0, secondary_weight=0.1):\n",
    "        model = cls(roberta_model_name, num_labels, primary_weight, secondary_weight)\n",
    "        model.load_state_dict(torch.load(os.path.join(save_directory, 'pytorch_model.bin')))\n",
    "        return model\n",
    "\n",
    "# Initialize the model\n",
    "model = RobertaForMultilabelRegression('roberta-base', num_labels=10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b5e590-7cc4-4533-8739-fb47dfd65de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/y0/5n71953n2_76_0v3gh0g_rxr0000gn/T/ipykernel_43792/3064770405.py:19: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Evaluation Loss: 0.20008041337132454\n",
      "Epoch 2/10, Evaluation Loss: 0.1779889091849327\n",
      "Epoch 3/10, Evaluation Loss: 0.19569828659296035\n",
      "Epoch 4/10, Evaluation Loss: 0.17517112493515014\n",
      "Epoch 5/10, Evaluation Loss: 0.17760688662528992\n",
      "Epoch 6/10, Evaluation Loss: 0.17577214017510415\n",
      "Epoch 7/10, Evaluation Loss: 0.17948233410716058\n",
      "Epoch 8/10, Evaluation Loss: 0.18440624549984933\n",
      "Epoch 9/10, Evaluation Loss: 0.17658943831920623\n",
      "Epoch 10/10, Evaluation Loss: 0.17892688065767287\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Initialize the model\n",
    "model = RobertaForMultilabelRegression('roberta-base', num_labels=10, primary_weight=1.0, secondary_weight=0.1)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.002,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',  # Directory for logging\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.9)\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(training_args.device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        batch = {k: v.to(training_args.device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            eval_loss += outputs[0].item()\n",
    "    eval_loss /= len(eval_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, Evaluation Loss: {eval_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fcaf0d-87bb-4a76-8022-671030d126db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-roberta/tokenizer_config.json',\n",
       " 'fine-tuned-roberta/special_tokens_map.json',\n",
       " 'fine-tuned-roberta/vocab.json',\n",
       " 'fine-tuned-roberta/merges.txt',\n",
       " 'fine-tuned-roberta/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "import os\n",
    "model.save_model(\"fine-tuned-roberta\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e1f0-4f9b-4e00-b0c1-1ab89a15a622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
