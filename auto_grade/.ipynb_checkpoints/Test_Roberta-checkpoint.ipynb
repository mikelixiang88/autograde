{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fa96e5-16d7-4747-ab50-030b8746f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# Define custom weighted loss (assume you have implemented this class)\n",
    "class CustomWeightedLoss(nn.Module):\n",
    "    def __init__(self, primary_weight, secondary_weight):\n",
    "        super(CustomWeightedLoss, self).__init__()\n",
    "        self.primary_weight = primary_weight\n",
    "        self.secondary_weight = secondary_weight\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Implement your custom loss calculation here\n",
    "        pass\n",
    "\n",
    "# Define the model\n",
    "class RobertaForMultilabelRegression(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_labels, primary_weight=1.0, secondary_weight=0.1):\n",
    "        super(RobertaForMultilabelRegression, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fn = CustomWeightedLoss(primary_weight, secondary_weight)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0][:, 0, :]  # Take <s> token (equiv. to [CLS])\n",
    "        logits = self.regressor(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, save_directory, roberta_model_name, num_labels, primary_weight=1.0, secondary_weight=0.1):\n",
    "        model = cls(roberta_model_name, num_labels, primary_weight, secondary_weight)\n",
    "        model.load_state_dict(torch.load(f\"{save_directory}/pytorch_model.bin\"))\n",
    "        return model\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForMultilabelRegression.load_model(\"fine-tuned-roberta\", 'roberta-base', num_labels=10)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"fine-tuned-roberta\")\n",
    "true_avg_scores = {'correctness_score': [], 'logic_score': [], 'truthfulness_score': []}\n",
    "true_std_scores = {'correctness_score': [], 'logic_score': [], 'truthfulness_score': []}\n",
    "predicted_scores = {'correctness_score': [], 'logic_score': [], 'truthfulness_score': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b9fc3d-73a3-427b-872d-69b31ac9a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (668) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 668].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 63\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Assuming the first three labels are correctness, logic, and truthfulness scores\u001b[39;00m\n\u001b[1;32m     66\u001b[0m predicted_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrectness_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(logits[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mRobertaForMultilabelRegression.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 26\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Take <s> token (equiv. to [CLS])\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(sequence_output)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:801\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    800\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 801\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (668) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 668].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "# Example usage: Perform inference\n",
    "import json\n",
    "# Load your dataset\n",
    "# Load the JSON files\n",
    "with open('test_data.json', 'r') as test_file, open('score_dis.json', 'r') as score_dis_file:\n",
    "    test_data = json.load(test_file)\n",
    "    score_dis_data = json.load(score_dis_file)\n",
    "\n",
    "# Create dictionaries for grader statistics\n",
    "grader_stats = {}\n",
    "for grader in score_dis_data:\n",
    "    grader_name = grader['grader']\n",
    "    grader_stats[grader_name] = {\n",
    "        'correctness_score': grader['correctness_score'],\n",
    "        'logic_score': grader['logic_score'],\n",
    "        'truthfulness_score': grader['truthfulness_score']\n",
    "    }\n",
    "\n",
    "# Initialize dictionaries to hold the scores\n",
    "score_sums = {}\n",
    "score_counts = {}\n",
    "\n",
    "# Initialize dictionaries to hold the standardized score sums\n",
    "standardized_sums = {}\n",
    "standardized_counts = {}\n",
    "\n",
    "# Iterate over each item in the data\n",
    "for item in test_data['data']:\n",
    "    displayed_text = item['displayed_text']\n",
    "    grader = item['grader']\n",
    "    scores = ['correctness_score', 'logic_score', 'truthfulness_score']\n",
    "    \n",
    "    if displayed_text not in score_sums:\n",
    "        score_sums[displayed_text] = {score: 0 for score in scores}\n",
    "        score_counts[displayed_text] = {score: 0 for score in scores}\n",
    "        standardized_sums[displayed_text] = {score: 0 for score in scores}\n",
    "        standardized_counts[displayed_text] = {score: 0 for score in scores}\n",
    "\n",
    "    for score in scores:\n",
    "        score_sums[displayed_text][score] += item[score]\n",
    "        score_counts[displayed_text][score] += 1\n",
    "        \n",
    "        mean_score = grader_stats[grader][score]['mean']\n",
    "        std_dev_score = grader_stats[grader][score]['std_dev']\n",
    "        standardized_score = (item[score] - mean_score) / std_dev_score\n",
    "        standardized_sums[displayed_text][score] += standardized_score\n",
    "        standardized_counts[displayed_text][score] += 1\n",
    "\n",
    "# Compute the average scores and rescale the standardized scores\n",
    "average_scores = {}\n",
    "rescaled_scores = {}\n",
    "for text in score_sums:\n",
    "    average_scores[text] = {score: score_sums[text][score] / score_counts[text][score] for score in score_sums[text]}\n",
    "    rescaled_scores[text] = {\n",
    "        score: (\n",
    "            (standardized_sums[text][score] / standardized_counts[text][score]) * grader_stats['abram'][score]['std_dev']\n",
    "            + grader_stats['abram'][score]['mean']\n",
    "        ) for score in standardized_sums[text]\n",
    "    }\n",
    "    # Get model predictions\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)\n",
    "\n",
    "    # Assuming the first three labels are correctness, logic, and truthfulness scores\n",
    "    predicted_scores['correctness_score'].append(logits[0, 0].item())\n",
    "    predicted_scores['logic_score'].append(logits[0, 1].item())\n",
    "    predicted_scores['truthfulness_score'].append(logits[0, 2].item())\n",
    "    \n",
    "    # Append true scores for MSE calculation\n",
    "    true_avg_scores['correctness_score'].append(average_scores[text]['correctness_score'])\n",
    "    true_avg_scores['logic_score'].append(average_scores[text]['logic_score'])\n",
    "    true_avg_scores['truthfulness_score'].append(average_scores[text]['truthfulness_score'])\n",
    "    true_std_scores['correctness_score'].append(rescaled_scores[text]['correctness_score'])\n",
    "    true_std_scores['logic_score'].append(rescaled_scores[text]['logic_score'])\n",
    "    true_std_scores['truthfulness_score'].append(rescaled_scores[text]['truthfulness_score'])\n",
    "print(predicted_scores)\n",
    "\"\"\"# Print the average and rescaled scores\n",
    "for text in average_scores:\n",
    "    print(f\"Displayed Text: {text}\")\n",
    "    print(\"  Average Scores:\")\n",
    "    for score, avg in average_scores[text].items():\n",
    "        print(f\"    {score}: {avg}\")\n",
    "    print(\"  Rescaled Standardized Scores:\")\n",
    "    for score, rescaled in rescaled_scores[text].items():\n",
    "        print(f\"    {score}: {rescaled}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2298a-96f9-412e-81d9-90afe240daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE for average and standardized scores\n",
    "mse_avg = {score: mean_squared_error(true_avg_scores[score], predicted_scores[score]) for score in true_avg_scores}\n",
    "mse_std = {score: mean_squared_error(true_std_scores[score], predicted_scores[score]) for score in true_std_scores}\n",
    "\n",
    "# Print MSE results\n",
    "print(\"MSE for average scores:\")\n",
    "for score, mse in mse_avg.items():\n",
    "    print(f\"  {score}: {mse}\")\n",
    "\n",
    "print(\"\\nMSE for standardized scores:\")\n",
    "for score, mse in mse_std.items():\n",
    "    print(f\"  {score}: {mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
