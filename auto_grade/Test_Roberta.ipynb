{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fa96e5-16d7-4747-ab50-030b8746f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "# Define custom weighted loss (assume you have implemented this class)\n",
    "class CustomWeightedLoss(nn.Module):\n",
    "    def __init__(self, primary_weight, secondary_weight):\n",
    "        super(CustomWeightedLoss, self).__init__()\n",
    "        self.primary_weight = primary_weight\n",
    "        self.secondary_weight = secondary_weight\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Implement your custom loss calculation here\n",
    "        pass\n",
    "\n",
    "# Define the model\n",
    "class RobertaForMultilabelRegression(nn.Module):\n",
    "    def __init__(self, roberta_model_name, num_labels, primary_weight=1.0, secondary_weight=0.1):\n",
    "        super(RobertaForMultilabelRegression, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fn = CustomWeightedLoss(primary_weight, secondary_weight)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0][:, 0, :]  # Take <s> token (equiv. to [CLS])\n",
    "        logits = self.regressor(sequence_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, save_directory, roberta_model_name, num_labels, primary_weight=1.0, secondary_weight=0.1):\n",
    "        model = cls(roberta_model_name, num_labels, primary_weight, secondary_weight)\n",
    "        model.load_state_dict(torch.load(f\"{save_directory}/pytorch_model.bin\"))\n",
    "        return model\n",
    "\n",
    "# Load the model\n",
    "model = RobertaForMultilabelRegression.load_model(\"fine-tuned-roberta\", 'roberta-base', num_labels=10)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"fine-tuned-roberta\")\n",
    "true_avg_scores = {'correctness_score': [], 'logic_score': [], 'truthfulness_score': []}\n",
    "true_std_scores = {'correctness_score': [], 'logic_score': [], 'truthfulness_score': []}\n",
    "predicted_scores = {'correctness_score': [], 'logic_score': [], 'truthfulness_score': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b9fc3d-73a3-427b-872d-69b31ac9a60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correctness_score': [4.362725257873535, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725734710693, 4.362725734710693, 4.362725734710693, 4.362725734710693, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725257873535, 4.362725257873535, 4.362726211547852, 4.362726211547852, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725734710693, 4.362725257873535, 4.362725257873535, 4.362725257873535, 4.362725734710693], 'logic_score': [4.599810600280762, 4.599810600280762, 4.599810600280762, 4.5998101234436035, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.5998101234436035, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.5998101234436035, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.5998101234436035, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.5998101234436035, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.59981107711792, 4.59981107711792, 4.599810600280762, 4.599810600280762, 4.5998101234436035, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762, 4.599810600280762], 'truthfulness_score': [4.806377410888672, 4.806377410888672, 4.806377410888672, 4.8063764572143555, 4.806377410888672, 4.806376934051514, 4.806377410888672, 4.8063764572143555, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.8063764572143555, 4.806377410888672, 4.806376934051514, 4.806377410888672, 4.8063764572143555, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806376934051514, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672, 4.806377410888672]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Print the average and rescaled scores\\nfor text in average_scores:\\n    print(f\"Displayed Text: {text}\")\\n    print(\"  Average Scores:\")\\n    for score, avg in average_scores[text].items():\\n        print(f\"    {score}: {avg}\")\\n    print(\"  Rescaled Standardized Scores:\")\\n    for score, rescaled in rescaled_scores[text].items():\\n        print(f\"    {score}: {rescaled}\")\\n    print()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage: Perform inference\n",
    "import json\n",
    "# Load your dataset\n",
    "# Load the JSON files\n",
    "with open('test_data.json', 'r') as test_file, open('score_dis.json', 'r') as score_dis_file:\n",
    "    test_data = json.load(test_file)\n",
    "    score_dis_data = json.load(score_dis_file)\n",
    "\n",
    "# Create dictionaries for grader statistics\n",
    "grader_stats = {}\n",
    "for grader in score_dis_data:\n",
    "    grader_name = grader['grader']\n",
    "    grader_stats[grader_name] = {\n",
    "        'correctness_score': grader['correctness_score'],\n",
    "        'logic_score': grader['logic_score'],\n",
    "        'truthfulness_score': grader['truthfulness_score']\n",
    "    }\n",
    "\n",
    "# Initialize dictionaries to hold the scores\n",
    "score_sums = {}\n",
    "score_counts = {}\n",
    "\n",
    "# Initialize dictionaries to hold the standardized score sums\n",
    "standardized_sums = {}\n",
    "standardized_counts = {}\n",
    "\n",
    "# Iterate over each item in the data\n",
    "for item in test_data['data']:\n",
    "    displayed_text = item['displayed_text']\n",
    "    grader = item['grader']\n",
    "    scores = ['correctness_score', 'logic_score', 'truthfulness_score']\n",
    "    \n",
    "    if displayed_text not in score_sums:\n",
    "        score_sums[displayed_text] = {score: 0 for score in scores}\n",
    "        score_counts[displayed_text] = {score: 0 for score in scores}\n",
    "        standardized_sums[displayed_text] = {score: 0 for score in scores}\n",
    "        standardized_counts[displayed_text] = {score: 0 for score in scores}\n",
    "\n",
    "    for score in scores:\n",
    "        score_sums[displayed_text][score] += item[score]\n",
    "        score_counts[displayed_text][score] += 1\n",
    "        \n",
    "        mean_score = grader_stats[grader][score]['mean']\n",
    "        std_dev_score = grader_stats[grader][score]['std_dev']\n",
    "        standardized_score = (item[score] - mean_score) / std_dev_score\n",
    "        standardized_sums[displayed_text][score] += standardized_score\n",
    "        standardized_counts[displayed_text][score] += 1\n",
    "\n",
    "# Compute the average scores and rescale the standardized scores\n",
    "average_scores = {}\n",
    "rescaled_scores = {}\n",
    "for text in score_sums:\n",
    "    average_scores[text] = {score: score_sums[text][score] / score_counts[text][score] for score in score_sums[text]}\n",
    "    rescaled_scores[text] = {\n",
    "        score: (\n",
    "            (standardized_sums[text][score] / standardized_counts[text][score]) * grader_stats['abram'][score]['std_dev']\n",
    "            + grader_stats['abram'][score]['mean']\n",
    "        ) for score in standardized_sums[text]\n",
    "    }\n",
    "    # Get model predictions\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)\n",
    "\n",
    "    # Assuming the first three labels are correctness, logic, and truthfulness scores\n",
    "    predicted_scores['correctness_score'].append(logits[0, 0].item())\n",
    "    predicted_scores['logic_score'].append(logits[0, 1].item())\n",
    "    predicted_scores['truthfulness_score'].append(logits[0, 2].item())\n",
    "    \n",
    "    # Append true scores for MSE calculation\n",
    "    true_avg_scores['correctness_score'].append(average_scores[text]['correctness_score'])\n",
    "    true_avg_scores['logic_score'].append(average_scores[text]['logic_score'])\n",
    "    true_avg_scores['truthfulness_score'].append(average_scores[text]['truthfulness_score'])\n",
    "    true_std_scores['correctness_score'].append(rescaled_scores[text]['correctness_score'])\n",
    "    true_std_scores['logic_score'].append(rescaled_scores[text]['logic_score'])\n",
    "    true_std_scores['truthfulness_score'].append(rescaled_scores[text]['truthfulness_score'])\n",
    "print(predicted_scores)\n",
    "\"\"\"# Print the average and rescaled scores\n",
    "for text in average_scores:\n",
    "    print(f\"Displayed Text: {text}\")\n",
    "    print(\"  Average Scores:\")\n",
    "    for score, avg in average_scores[text].items():\n",
    "        print(f\"    {score}: {avg}\")\n",
    "    print(\"  Rescaled Standardized Scores:\")\n",
    "    for score, rescaled in rescaled_scores[text].items():\n",
    "        print(f\"    {score}: {rescaled}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b2298a-96f9-412e-81d9-90afe240daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for average scores:\n",
      "  correctness_score: 0.6233282182366755\n",
      "  logic_score: 0.31678505168532756\n",
      "  truthfulness_score: 0.48498183011116236\n",
      "\n",
      "MSE for standardized scores:\n",
      "  correctness_score: 0.7517898113443472\n",
      "  logic_score: 0.38444711712395524\n",
      "  truthfulness_score: 0.5583725400733515\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE for average and standardized scores\n",
    "import numpy as np\n",
    "def mean_squared_error(true_scores, predicted_scores):\n",
    "    true_scores = np.array(true_scores)\n",
    "    predicted_scores = np.array(predicted_scores)\n",
    "    mse = np.mean((true_scores - predicted_scores) ** 2)\n",
    "    return mse\n",
    "mse_avg = {score: mean_squared_error(true_avg_scores[score], predicted_scores[score]) for score in true_avg_scores}\n",
    "mse_std = {score: mean_squared_error(true_std_scores[score], predicted_scores[score]) for score in true_std_scores}\n",
    "\n",
    "# Print MSE results\n",
    "print(\"MSE for average scores:\")\n",
    "for score, mse in mse_avg.items():\n",
    "    print(f\"  {score}: {mse}\")\n",
    "\n",
    "print(\"\\nMSE for standardized scores:\")\n",
    "for score, mse in mse_std.items():\n",
    "    print(f\"  {score}: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3e679-8e3f-455c-b954-e36bc29b5305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
