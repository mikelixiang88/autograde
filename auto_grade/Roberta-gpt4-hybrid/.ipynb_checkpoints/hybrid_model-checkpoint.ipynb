{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846d2fd8-92df-4055-88fc-a36499dc24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer, DataCollatorWithPadding\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"train_data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "# Convert the dataset to the Hugging Face datasets format\n",
    "dataset_dict = {\n",
    "    \"text\": [entry[\"displayed_text\"] for entry in data],\n",
    "    \"scores_labels\": [[0.0 if v is None else float(v) for v in [\n",
    "        entry[\"correctness_score\"], entry[\"logic_score\"], entry[\"truthfulness_score\"]\n",
    "    ]] for entry in data],\n",
    "    \"grader\": [entry[\"grader\"] for entry in data],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c416944-22e2-44ee-995b-a50851fa8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define function chatgpt grading\n",
    "def generate_score(text):\n",
    "    response=client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.8,\n",
    "        max_tokens=800,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are reviewing response to physics questions. You will be given a question, a response, \n",
    "            and a ground truth. Using the ground truth as reference, comment on the response in terms of the presence of calculation error, \n",
    "            hallucination error, irrelevancy, and logic error. Be objective and comprehensive, but keep it concise. You must keep your \n",
    "            response within 200 words at most.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "comment_dict={}\n",
    "\n",
    "for entry in dataset_dict[\"text\"]:\n",
    "    if entry in comment_dict:\n",
    "        continue\n",
    "    else:\n",
    "        response=generate_score(entry)\n",
    "        comment_dict[entry]=response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd889dbe-9648-4b28-b60c-745deb44c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open(\"gpt-comment.json\", 'w') as file:\n",
    "    json.dump(comment_dict, file, indent=4)\"\"\"\n",
    "with open(\"gpt-comment.json\", 'r') as file:\n",
    "    comment_dict=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b454b0ec-9650-4ff9-a9b1-9b8850877b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unique graders\n",
    "unique_graders = set(dataset_dict[\"grader\"])\n",
    "\n",
    "# Splitting the dataset by grader\n",
    "datasets_by_grader = {grader: {\"text\": [], \"scores_labels\": [], \"grader\": []} for grader in unique_graders}\n",
    "\n",
    "# Populate the data for each grader\n",
    "for i, grader in enumerate(dataset_dict[\"grader\"]):\n",
    "    datasets_by_grader[grader][\"text\"].append(dataset_dict[\"text\"][i])\n",
    "    datasets_by_grader[grader][\"scores_labels\"].append(dataset_dict[\"scores_labels\"][i])\n",
    "    datasets_by_grader[grader][\"grader\"].append(grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7fc5a1f-01bd-47e0-95fa-96d8f37b63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, RobertaModel\n",
    "\n",
    "# Define the model\n",
    "class RobertaForMultilabelRegression(nn.Module):\n",
    "    def __init__(self, roberta_model_name: str, num_labels: int):\n",
    "        super(RobertaForMultilabelRegression, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction='mean')  # or 'none' if you prefer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0][:, 0, :]  # Take <s> token (equiv. to [CLS])\n",
    "        logits = self.regressor(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    def save_model(self, save_directory: str):\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, 'pytorch_model.bin'))\n",
    "        with open(os.path.join(save_directory, 'config.json'), 'w') as f:\n",
    "            f.write(self.roberta.config.to_json_string())\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, save_directory: str, roberta_model_name: str, num_labels: int):\n",
    "        model = cls(roberta_model_name, num_labels)\n",
    "        model.load_state_dict(torch.load(os.path.join(save_directory, 'pytorch_model.bin')))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b07b2e-6722-414b-85d2-35740a1f3ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function tokenize_function at 0x3299e3250> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8173c89df834eb182153250472fdc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 58\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Assuming comment_dict is properly defined and accessible here\n",
    "    texts = [comment_dict.get(key, \"\")[:500] for key in examples[\"text\"]]\n",
    "    # Tokenize texts while ensuring padding and truncation\n",
    "    tokenized_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = examples[\"scores_labels\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "for grader in unique_graders:\n",
    "    dataset = Dataset.from_dict(datasets_by_grader[grader])\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    split_dataset = tokenized_datasets.train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "    train_dataset = split_dataset[\"train\"].remove_columns(['text', 'grader', 'scores_labels'])\n",
    "    eval_dataset = split_dataset[\"test\"].remove_columns(['text', 'grader', 'scores_labels'])\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "    \n",
    "        \n",
    "    model = RobertaForMultilabelRegression('roberta-base', num_labels=3)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{grader}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=0.002,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_{grader}',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.8)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                eval_loss += outputs[0].item()\n",
    "        eval_loss /= len(eval_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, Evaluation Loss: {eval_loss}\")\n",
    "\n",
    "    model.save_model(f\"fine-tuned-roberta_{grader}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e1f0-4f9b-4e00-b0c1-1ab89a15a622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
