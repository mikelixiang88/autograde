{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fa96e5-16d7-4747-ab50-030b8746f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "graders=['ryan', 'haoran', 'abram', 'ziwei', 'louis', 'mikeli']\n",
    "class RobertaForMultilabelRegression(nn.Module):\n",
    "    def __init__(self, roberta_model_name: str, num_labels: int):\n",
    "        super(RobertaForMultilabelRegression, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction='mean')  # or 'none' if you prefer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0][:, 0, :]  # Take <s> token (equiv. to [CLS])\n",
    "        logits = self.regressor(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    def save_model(self, save_directory: str):\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, 'pytorch_model.bin'))\n",
    "        with open(os.path.join(save_directory, 'config.json'), 'w') as f:\n",
    "            f.write(self.roberta.config.to_json_string())\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, save_directory: str, roberta_model_name: str, num_labels: int):\n",
    "        model = cls(roberta_model_name, num_labels)\n",
    "        model.load_state_dict(torch.load(os.path.join(save_directory, 'pytorch_model.bin')))\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0baa6e32-05ba-4384-847f-809c93ce65f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "def generate_score(text):\n",
    "    response=client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.8,\n",
    "        max_tokens=800,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are reviewing response to physics questions. You will be given a question, a response, \n",
    "            and a ground truth. Using the ground truth as reference, comment on the response in terms of the presence of calculation error, \n",
    "            hallucination error, irrelevancy, and logic error. Be objective and comprehensive, but keep it concise. You must keep your \n",
    "            response within 200 words at most.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "    \n",
    "with open('test_data.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "\n",
    "models={}\n",
    "for grader in graders:\n",
    "    models[grader]=RobertaForMultilabelRegression.load_model(\"fine-tuned-roberta_\"+grader, 'roberta-base', num_labels=3)\n",
    "    models[grader].eval()\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"fine-tuned-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9470ce45-d027-4e0b-aacd-24d0500e5d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        correctness_score  logic_score  truthfulness_score\n",
      "ryan             1.246545     1.682893            1.694764\n",
      "haoran           1.559733     0.640667            1.921097\n",
      "abram            1.409028     0.417089            0.985830\n",
      "ziwei            2.287383     1.543196            1.505417\n",
      "louis            1.121188     0.556699            0.351259\n",
      "mikeli           0.590309     0.499870            0.094581\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Initialize dictionaries to hold the scores\n",
    "score_sums = {}\n",
    "score_counts = {}\n",
    "pred_sums={}\n",
    "texts=[]\n",
    "model_mse={'ryan':{'correctness_score':0.0, 'logic_score':0.0, 'truthfulness_score':0.0},\n",
    "           'haoran':{'correctness_score':0.0, 'logic_score':0.0, 'truthfulness_score':0.0},\n",
    "           'abram':{'correctness_score':0.0, 'logic_score':0.0, 'truthfulness_score':0.0},\n",
    "           'ziwei':{'correctness_score':0.0, 'logic_score':0.0, 'truthfulness_score':0.0},\n",
    "           'louis':{'correctness_score':0.0, 'logic_score':0.0, 'truthfulness_score':0.0},\n",
    "           'mikeli':{'correctness_score':0.0, 'logic_score':0.0, 'truthfulness_score':0.0}}\n",
    "grader_count={'ryan':0, 'haoran':0, 'abram':0, 'ziwei':0, 'louis':0, 'mikeli':0}\n",
    "\n",
    "# Iterate over each item in the data\n",
    "for item in test_data['data']:\n",
    "    displayed_text = item['displayed_text']\n",
    "    grader = item['grader']\n",
    "    grader_count[grader]+=1\n",
    "    scores = ['correctness_score', 'logic_score', 'truthfulness_score']\n",
    "    text=generate_score(displayed_text)\n",
    "    texts.append(text)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = models[grader](**inputs)\n",
    "    \n",
    "    pred={'correctness_score':logits[0, 0].item(), 'logic_score': logits[0, 1].item(), 'truthfulness_score': logits[0, 2].item()}\n",
    "    for score in scores:\n",
    "        model_mse[grader][score]+=((pred[score]-item[score])**2)\n",
    "    \n",
    "    if displayed_text not in score_sums:\n",
    "        score_sums[displayed_text] = {score: 0 for score in scores}\n",
    "        score_counts[displayed_text] = {score: 0 for score in scores}\n",
    "        pred_sums[displayed_text] = {score: 0 for score in scores}\n",
    "\n",
    "    for score in scores:\n",
    "        score_sums[displayed_text][score] += item[score]\n",
    "        score_counts[displayed_text][score] += 1\n",
    "        pred_sums[displayed_text][score]+= pred[score]\n",
    "\n",
    "for grader in graders:\n",
    "    for key in model_mse[grader]:\n",
    "        model_mse[grader][key]/=grader_count[grader]\n",
    "\n",
    "df = pd.DataFrame(model_mse).T  # Transpose to switch rows and columns\n",
    "print(df)\n",
    "\n",
    "# Dump the output gpt-4 comments\n",
    "with open('gpt_comment_test.json', 'w') as test_output:\n",
    "    json.dump(texts, test_output, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b95ca57-34fc-467e-83b0-685dd0f66007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correctness_score': 0.2415935747882112, 'logic_score': 0.14646256179636497, 'truthfulness_score': 0.17154164194284982}\n"
     ]
    }
   ],
   "source": [
    "# Compute the average scores and rescale the standardized scores\n",
    "average_scores = {}\n",
    "avg_pred = {}\n",
    "\n",
    "for text in score_sums:\n",
    "    average_scores[text] = {score: score_sums[text][score] / score_counts[text][score] for score in score_sums[text]}\n",
    "    avg_pred[text]= {score: pred_sums[text][score] / score_counts[text][score] for score in pred_sums[text]}\n",
    "\n",
    "mse={\"correctness_score\":0.0, \"logic_score\":0.0, \"truthfulness_score\":0.0}\n",
    "count=0\n",
    "for text in score_sums:\n",
    "    for score in score_sums[text]:\n",
    "        mse[score]+=(avg_pred[text][score]-average_scores[text][score])**2\n",
    "        count+=1\n",
    "\n",
    "for key in mse:\n",
    "    mse[key]/=count\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d229be-4b00-4cc0-ad11-899db877e246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
