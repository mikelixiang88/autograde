{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846d2fd8-92df-4055-88fc-a36499dc24a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer, DataCollatorWithPadding\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"train_data.json\", \"r\") as f:\n",
    "    data = json.load(f)[\"data\"]\n",
    "\n",
    "# Convert the dataset to the Hugging Face datasets format\n",
    "dataset_dict = {\n",
    "    \"text\": [entry[\"displayed_text\"] for entry in data],\n",
    "    \"scores_labels\": [[0.0 if v is None else float(v) for v in [\n",
    "        entry[\"correctness_score\"], entry[\"logic_score\"], entry[\"truthfulness_score\"]\n",
    "    ]] for entry in data],\n",
    "    \"grader\": [entry[\"grader\"] for entry in data],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c416944-22e2-44ee-995b-a50851fa8ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define function chatgpt grading\n",
    "def generate_score(text):\n",
    "    response=client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.8,\n",
    "        max_tokens=800,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You are reviewing response to physics questions. You will be given a question, a response, \n",
    "            and a ground truth. Using the ground truth as reference, comment on the response in terms of the presence of calculation error, \n",
    "            hallucination error, irrelevancy, and logic error. Be objective and comprehensive, but keep it concise. You must keep your \n",
    "            response within 200 words at most.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "comment_dict={}\n",
    "\n",
    "for entry in dataset_dict[\"text\"]:\n",
    "    if entry in comment_dict:\n",
    "        continue\n",
    "    else:\n",
    "        response=generate_score(entry)\n",
    "        comment_dict[entry]=response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd889dbe-9648-4b28-b60c-745deb44c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"with open(\"gpt-comment.json\", 'w') as file:\n",
    "    json.dump(comment_dict, file, indent=4)\"\"\"\n",
    "with open(\"gpt-comment.json\", 'r') as file:\n",
    "    comment_dict=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b454b0ec-9650-4ff9-a9b1-9b8850877b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding unique graders\n",
    "unique_graders = set(dataset_dict[\"grader\"])\n",
    "\n",
    "# Splitting the dataset by grader\n",
    "datasets_by_grader = {grader: {\"text\": [], \"scores_labels\": [], \"grader\": []} for grader in unique_graders}\n",
    "\n",
    "# Populate the data for each grader\n",
    "for i, grader in enumerate(dataset_dict[\"grader\"]):\n",
    "    datasets_by_grader[grader][\"text\"].append(dataset_dict[\"text\"][i])\n",
    "    datasets_by_grader[grader][\"scores_labels\"].append(dataset_dict[\"scores_labels\"][i])\n",
    "    datasets_by_grader[grader][\"grader\"].append(grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7fc5a1f-01bd-47e0-95fa-96d8f37b63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import TrainingArguments, RobertaModel\n",
    "\n",
    "# Define the model\n",
    "class RobertaForMultilabelRegression(nn.Module):\n",
    "    def __init__(self, roberta_model_name: str, num_labels: int):\n",
    "        super(RobertaForMultilabelRegression, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = RobertaModel.from_pretrained(roberta_model_name)\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        self.loss_fn = nn.SmoothL1Loss(reduction='mean')  # or 'none' if you prefer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0][:, 0, :]  # Take <s> token (equiv. to [CLS])\n",
    "        logits = self.regressor(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "    def save_model(self, save_directory: str):\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, 'pytorch_model.bin'))\n",
    "        with open(os.path.join(save_directory, 'config.json'), 'w') as f:\n",
    "            f.write(self.roberta.config.to_json_string())\n",
    "\n",
    "    @classmethod\n",
    "    def load_model(cls, save_directory: str, roberta_model_name: str, num_labels: int):\n",
    "        model = cls(roberta_model_name, num_labels)\n",
    "        model.load_state_dict(torch.load(os.path.join(save_directory, 'pytorch_model.bin')))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8f85b2-1bd4-406c-93cd-52dc0c425ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function tokenize_function at 0x1058c11b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299e340cfd34449aaf98aa0d4810602b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Evaluation Loss: 0.893435001373291\n",
      "Epoch 2/6, Evaluation Loss: 1.001727283000946\n",
      "Epoch 3/6, Evaluation Loss: 0.9255439639091492\n",
      "Epoch 4/6, Evaluation Loss: 1.010292410850525\n",
      "Epoch 5/6, Evaluation Loss: 0.98626509308815\n",
      "Epoch 6/6, Evaluation Loss: 0.9446495771408081\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11bd6751a8146dfa01aa0006b49a22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Evaluation Loss: 0.695886492729187\n",
      "Epoch 2/6, Evaluation Loss: 1.0352841019630432\n",
      "Epoch 3/6, Evaluation Loss: 0.7124559283256531\n",
      "Epoch 4/6, Evaluation Loss: 0.6837186217308044\n",
      "Epoch 5/6, Evaluation Loss: 0.6802806854248047\n",
      "Epoch 6/6, Evaluation Loss: 0.8791805505752563\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc62f38aec5a47f2adc511c17e2cdcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Evaluation Loss: 0.8284439146518707\n",
      "Epoch 2/6, Evaluation Loss: 0.7024035155773163\n",
      "Epoch 3/6, Evaluation Loss: 0.723159521818161\n",
      "Epoch 4/6, Evaluation Loss: 0.6798948496580124\n",
      "Epoch 5/6, Evaluation Loss: 0.715583473443985\n",
      "Epoch 6/6, Evaluation Loss: 0.7225950956344604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d13ef7250c4090b7456dd1cacf3b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Evaluation Loss: 0.6494656950235367\n",
      "Epoch 2/6, Evaluation Loss: 0.6053258031606674\n",
      "Epoch 3/6, Evaluation Loss: 0.5281317383050919\n",
      "Epoch 4/6, Evaluation Loss: 0.5143789798021317\n",
      "Epoch 5/6, Evaluation Loss: 0.5616019368171692\n",
      "Epoch 6/6, Evaluation Loss: 0.5403618514537811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651b3875482540bcbc9875466e610cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Evaluation Loss: 0.804334819316864\n",
      "Epoch 2/6, Evaluation Loss: 0.6968740671873093\n",
      "Epoch 3/6, Evaluation Loss: 0.6437824815511703\n",
      "Epoch 4/6, Evaluation Loss: 0.6646958738565445\n",
      "Epoch 5/6, Evaluation Loss: 0.6389547288417816\n",
      "Epoch 6/6, Evaluation Loss: 0.6614431887865067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b55585f29f4c3fadcdf6db751e1c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, Evaluation Loss: 0.2492600530385971\n",
      "Epoch 2/6, Evaluation Loss: 0.3338034152984619\n",
      "Epoch 3/6, Evaluation Loss: 0.31757013499736786\n",
      "Epoch 4/6, Evaluation Loss: 0.23966208845376968\n",
      "Epoch 5/6, Evaluation Loss: 0.23144873976707458\n",
      "Epoch 6/6, Evaluation Loss: 0.2575795277953148\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Assuming comment_dict is properly defined and accessible here\n",
    "    texts = [comment_dict.get(key, \"\")[:500] for key in examples[\"text\"]]\n",
    "    # Tokenize texts while ensuring padding and truncation\n",
    "    tokenized_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_inputs[\"labels\"] = examples[\"scores_labels\"]\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "for grader in unique_graders:\n",
    "    dataset = Dataset.from_dict(datasets_by_grader[grader])\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    split_dataset = tokenized_datasets.train_test_split(test_size=0.1)  # 10% for validation\n",
    "\n",
    "    train_dataset = split_dataset[\"train\"].remove_columns(['text', 'grader', 'scores_labels'])\n",
    "    eval_dataset = split_dataset[\"test\"].remove_columns(['text', 'grader', 'scores_labels'])\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=data_collator)\n",
    "    \n",
    "        \n",
    "    model = RobertaForMultilabelRegression('roberta-base', num_labels=3)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results_{grader}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=0.001,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=6,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs_{grader}',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.8)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                eval_loss += outputs[0].item()\n",
    "        eval_loss /= len(eval_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{training_args.num_train_epochs}, Evaluation Loss: {eval_loss}\")\n",
    "\n",
    "    model.save_model(f\"fine-tuned-roberta_{grader}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa011ba-8282-4ac2-b6a3-9320705889fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-roberta/tokenizer_config.json',\n",
       " 'fine-tuned-roberta/special_tokens_map.json',\n",
       " 'fine-tuned-roberta/vocab.json',\n",
       " 'fine-tuned-roberta/merges.txt',\n",
       " 'fine-tuned-roberta/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"fine-tuned-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005edd86-5ed2-4683-b181-b2853731e87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
